import asyncio
import logging
from .celery_app import celery_app
from .smart_worker import SmartWorker
from src.db.database import AsyncSessionLocal
from src.db.models import Task, TaskStatus
from src.api.sse_manager import sse_manager

logger = logging.getLogger(__name__)

async def progress_callback(task_id: str, current_page: int, total_pages: int, progress: float, status: str):
    """è¿›åº¦å›è°ƒå‡½æ•° - é€šè¿‡SSEå‘é€è¿›åº¦æ›´æ–°"""
    try:
        await sse_manager.broadcast_progress(
            task_id=task_id,
            current_page=current_page,
            total_pages=total_pages,
            progress=progress,
            status=status
        )
        logger.debug(f"Progress update for task {task_id}: {progress:.1f}% ({current_page}/{total_pages})")
    except Exception as e:
        logger.error(f"Failed to send progress update: {e}")


async def run_async_process(task_id: str, input_path: str, model_name: str, concurrency: int = 2, api_key: str = None, base_url: str = None):
    logger.info(f"ğŸš€ Starting async process for task {task_id}")
    logger.info(f"ğŸ“‹ Task parameters:")
    logger.info(f"   - Input path: {input_path}")
    logger.info(f"   - Model: {model_name}")
    logger.info(f"   - Concurrency: {concurrency}")
    logger.info(f"   - API Key: {'âœ… Set' if api_key else 'âŒ NOT SET'}")
    logger.info(f"   - Base URL: {base_url or 'Not set'}")

    from datetime import datetime

    # 1. Update status to PROCESSING and set started_at
    async with AsyncSessionLocal() as session:
        task = await session.get(Task, task_id)
        if not task:
            logger.error(f"Task {task_id} not found")
            return

        task.status = TaskStatus.PROCESSING
        task.started_at = datetime.utcnow()
        await session.commit()
        logger.info(f"âœ… Task {task_id} status updated to PROCESSING at {task.started_at}")

    try:
        # 2. Run Worker with progress callback
        logger.info(f"ğŸ”§ Initializing SmartWorker...")
        worker = SmartWorker(
            model_name=model_name,
            concurrency=concurrency,
            api_key=api_key,
            base_url=base_url,
            progress_callback=progress_callback
        )
        logger.info(f"âœ… SmartWorker initialized, starting file processing...")
        markdown_content, total_pages, input_tokens, output_tokens = await worker.process_file(input_path, task_id=task_id)

        logger.info(f"âœ… Processing completed. Total pages: {total_pages}")
        logger.info(f"ğŸ“Š Token usage - Input: {input_tokens}, Output: {output_tokens}, Total: {input_tokens + output_tokens}")

        # 3. Save Result & Update COMPLETED
        # è¾“å‡ºæ–‡ä»¶ä½¿ç”¨åŸå§‹ PDF æ–‡ä»¶åï¼Œæ‰©å±•åæ”¹ä¸º .md
        import os
        pdf_dir = os.path.dirname(input_path)
        pdf_name = os.path.basename(input_path)
        md_name = os.path.splitext(pdf_name)[0] + ".md"
        output_file = os.path.join(pdf_dir, md_name)
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        async with AsyncSessionLocal() as session:
            task = await session.get(Task, task_id)
            if task:
                task.status = TaskStatus.COMPLETED
                task.total_pages = total_pages  # æ›´æ–°æ€»é¡µæ•°
                task.result_path = output_file
                task.completed_at = datetime.utcnow()  # è®¾ç½®å®Œæˆæ—¶é—´
                task.input_tokens = input_tokens  # æ›´æ–° token ç»Ÿè®¡
                task.output_tokens = output_tokens
                task.total_tokens = input_tokens + output_tokens
                await session.commit()

        logger.info(f"Task {task_id} completed successfully with {total_pages} pages")
        logger.info(f"â±ï¸  Duration: {task.completed_at - task.started_at}")

    except Exception as e:
        logger.error(f"Task {task_id} failed: {e}")
        async with AsyncSessionLocal() as session:
            task = await session.get(Task, task_id)
            if task:
                task.status = TaskStatus.FAILED
                task.error_message = str(e)
                await session.commit()
        raise e


async def regenerate_single_page(
    task_id: str,
    page_num: int,
    image_path: str,
    model_name: str,
    api_key: str = None,
    base_url: str = None
):
    """
    é‡æ–°ç”Ÿæˆå•ä¸ªé¡µé¢çš„ Markdown å†…å®¹ï¼Œå¹¶é‡æ–°åˆå¹¶æ‰€æœ‰é¡µé¢

    Args:
        task_id: ä»»åŠ¡ID
        page_num: é¡µç 
        image_path: é¡µé¢å›¾ç‰‡è·¯å¾„
        model_name: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
    """
    logger.info(f"ğŸ”„ Regenerating page {page_num} for task {task_id}")

    import os
    from pathlib import Path
    task_dir = Path(os.path.dirname(image_path))

    # ç”¨äºè·Ÿè¸ª token ä½¿ç”¨
    total_tokens_used = 0

    try:
        # åˆå§‹åŒ– SmartWorkerï¼ˆä¸éœ€è¦ progress_callbackï¼‰
        worker = SmartWorker(
            model_name=model_name,
            concurrency=1,  # å•é¡µå¹¶å‘ä¸º1
            api_key=api_key,
            base_url=base_url,
            progress_callback=None  # å•é¡µé‡æ–°ç”Ÿæˆä¸éœ€è¦è¿›åº¦å›è°ƒ
        )

        # 1. è½¬æ¢å•é¡µ - ä½¿ç”¨ _convert_one æ–¹æ³•è€Œä¸æ˜¯ process_file
        # _convert_one æ˜¯åŒæ­¥æ–¹æ³•ï¼Œéœ€è¦åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ
        import asyncio
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, worker._convert_one, image_path)

        # æå– markdown å†…å®¹å’Œ token ä½¿ç”¨
        if hasattr(result, 'content'):
            page_markdown = result.content
        else:
            page_markdown = str(result)

        # ç»Ÿè®¡ token ä½¿ç”¨
        if hasattr(result, 'total_tokens'):
            total_tokens_used += result.total_tokens
            logger.info(f"ğŸ“Š Page {page_num} used {result.total_tokens} tokens")

        # 2. ä¿å­˜å•é¡µ markdown æ–‡ä»¶ï¼ˆä½¿ç”¨åŸå­æ“ä½œï¼‰
        page_md_path = task_dir / f"page_{page_num:04d}.md"
        page_md_tmp = task_dir / f"page_{page_num:04d}.md.tmp"

        # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
        with open(page_md_tmp, "w", encoding="utf-8") as f:
            f.write(page_markdown)

        # åŸå­æ€§é‡å‘½åï¼ˆOS çº§åˆ«çš„åŸå­æ“ä½œï¼‰
        os.replace(page_md_tmp, page_md_path)

        logger.info(f"âœ… Page {page_num} markdown saved to {page_md_path}, now merging all pages...")

        # 3. æŸ¥æ‰¾æ‰€æœ‰é¡µé¢æ–‡ä»¶å¹¶æŒ‰é¡ºåºåˆå¹¶ï¼ˆä½¿ç”¨ä¸¥æ ¼çš„æ¨¡å¼åŒ¹é…ï¼‰
        # ä½¿ç”¨ä¸¥æ ¼çš„4ä½æ•°å­—æ¨¡å¼åŒ¹é…ï¼Œé¿å…åŒ¹é…åˆ°ç”¨æˆ·ä¸Šä¼ çš„å…¶ä»–æ–‡ä»¶
        page_files = sorted(task_dir.glob("page_[0-9][0-9][0-9][0-9].md"))
        if not page_files:
            raise Exception(f"No page files found in {task_dir}")

        logger.info(f"Found {len(page_files)} page files to merge")

        # 4. åˆå¹¶æ‰€æœ‰é¡µé¢
        merged_parts = []
        for page_file in page_files:
            try:
                with open(page_file, "r", encoding="utf-8") as f:
                    content = f.read()

                # æå–é¡µç ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†ï¼‰
                try:
                    page_num_from_file = int(page_file.stem.split("_")[1])
                    merged_parts.append((page_num_from_file, content))
                    logger.debug(f"Loaded page {page_num_from_file} from {page_file.name}")
                except (ValueError, IndexError) as e:
                    logger.error(f"Invalid page file format: {page_file.name}: {e}")
                    continue  # è·³è¿‡æ ¼å¼é”™è¯¯çš„æ–‡ä»¶

            except Exception as e:
                logger.error(f"Failed to read {page_file}: {e}")

        # æŒ‰é¡µç æ’åº
        merged_parts.sort(key=lambda x: x[0])
        logger.info(f"Merging pages in order: {[p[0] for p in merged_parts]}")

        # 5. æ„å»ºæœ€ç»ˆ markdownï¼ˆæ·»åŠ é¡µç åˆ†éš”ç¬¦ï¼‰
        final_markdown = ""
        for idx, (pnum, content) in enumerate(merged_parts):
            final_markdown += f"\n\n<!-- Page {pnum} -->\n\n"
            final_markdown += content
            if idx < len(merged_parts) - 1:
                final_markdown += "\n\n---\n\n"  # é¡µé¢åˆ†éš”ç¬¦

        # 6. ä¿å­˜åˆ°æœ€ç»ˆçš„åˆå¹¶æ–‡ä»¶
        # æŸ¥æ‰¾åŸå§‹ PDF æ–‡ä»¶å
        pdf_files = list(task_dir.glob("*.pdf"))
        if pdf_files:
            pdf_name = pdf_files[0].stem  # ä¸å«æ‰©å±•å
            output_file = task_dir / f"{pdf_name}.md"

            # ä½¿ç”¨åŸå­æ“ä½œä¿å­˜
            output_tmp = task_dir / f"{pdf_name}.md.tmp"
            with open(output_tmp, "w", encoding="utf-8") as f:
                f.write(final_markdown)
            os.replace(output_tmp, output_file)

            logger.info(f"âœ… Merged markdown saved to: {output_file}")
            logger.info(f"ğŸ“Š Total size: {len(final_markdown)} characters")
        else:
            logger.warning(f"âš ï¸  No PDF file found in {task_dir}, skipping merge")

        logger.info(f"âœ… Page {page_num} regeneration and merge completed")

        # 7. æ›´æ–°æ•°æ®åº“ä¸­çš„ token ç»Ÿè®¡
        if total_tokens_used > 0:
            from src.db.database import AsyncSessionLocal
            from src.db.models import Task

            async with AsyncSessionLocal() as session:
                task = await session.get(Task, task_id)
                if task:
                    # å¢åŠ åˆ°ç°æœ‰çš„ token è®¡æ•°
                    if task.total_tokens:
                        task.total_tokens += total_tokens_used
                    else:
                        task.total_tokens = total_tokens_used
                    await session.commit()
                    logger.info(f"ğŸ“Š Updated total_tokens for task {task_id}: {task.total_tokens}")

    except Exception as e:
        logger.error(f"âŒ Failed to regenerate page {page_num}: {e}")
        raise e

@celery_app.task(bind=True)
def convert_pdf_task(self, task_id: str, input_path: str, model_name: str = "gpt-4o", concurrency: int = 2, api_key: str = None, base_url: str = None):
    """
    Celery task to convert PDF to Markdown
    """
    # Run async function in blocking manner
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
    loop.run_until_complete(run_async_process(task_id, input_path, model_name, concurrency, api_key, base_url))
